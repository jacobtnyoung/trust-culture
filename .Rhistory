# Clear the workspace
rm( list = ls() )
# Load the libraries used
library( haven )
library( here )
library( reshape2 )
library( ggplot2 )
library( AnthroTools ) #install_github("alastair-JL/AnthroTools")
install_github("alastair-JL/AnthroTools")
library(devtools)
install_github("alastair-JL/AnthroTools")
here()
dat <- as.data.frame(
read_dta(
here::here( "trust-culture-data/PV PAR Interviews Full_CLEANED.dta" ) ) )
# Extract the trust variables
# S3SS1_1-S3SS1_15
trust.vars <- as.matrix( dat[,
which( colnames( dat ) == ( "S3SS1_1" ) ):
which( colnames( dat ) == ( "S3SS1_15" ) )
] )
# keep variables that are worded well
vars.keep <- c( "S3SS1_1", "S3SS1_3", "S3SS1_4", "S3SS1_7", "S3SS1_9", "S3SS1_10", "S3SS1_11", "S3SS1_12", "S3SS1_14" )
trust.vars2 <- ( trust.vars[, which( colnames( trust.vars ) %in%  vars.keep )  ] )
trust.vars  <- trust.vars2
# Clean up missing values and examine descriptive statistics
# Replace the missing values
NA.replace <- function( input.dat, pdist ){
set.seed( 12345 )
for( i in 1:nrow( input.dat ) ){
for( j in 1:ncol( input.dat ) ){
if( is.na( input.dat[i,j] == TRUE ) )
input.dat[i,j] <- pdist
}
}
return( input.dat )
}
# Run the function to replace the missing values
# replace 67 missing values: table( is.na( trust.vars ) )
trust.dat   <- NA.replace( trust.vars, round( runif( n = 1, min = 0, max = 1 ), 0 ) )
apply( trust.dat, 2, mean )
apply( trust.dat, 2, sd )
mean( apply( trust.dat, 2, mean ) )
sd( apply( trust.dat, 2, mean ) )
DiscountedAgreementMatrix <-
function( response.mat, n.responses ){
discounted.matrix= matrix( 0, nrow( response.mat ),nrow( response.mat ) )
for( i in 1:nrow( discounted.matrix ) ){
for( j in 1:ncol( discounted.matrix ) ){
discounted.matrix[i,j] <- mean( response.mat[i,] == response.mat[j,], na.rm = TRUE ) # Create the average number of items i and j agree on
}
}
discounted.matrix <- ( discounted.matrix*n.responses -1 ) / (n.responses-1 ) # Correct the average agreement for guessing
diag( discounted.matrix ) <- 0 # set the diagonal to zero
discounted.matrix[is.finite(discounted.matrix) == FALSE] <- 0 # adjust Inf values to zero (if any)
return( discounted.matrix )
}
trust.agreement <- DiscountedAgreementMatrix( trust.dat, 2 )
mean( trust.agreement )
# Create data for the plot.
dat.for.plot <- function( cormat ){
dd <- as.dist( ( 1-cormat )/2 ) # Use correlation between variables as distance
hc <- hclust( dd )
cormat <-cormat[hc$order, hc$order]
cormat[lower.tri( cormat )] <- NA # set the lower part of the triangle as missing (redundant information)
cor.dat <- melt( cormat, na.rm = TRUE ) # create the dataframe
return( cor.dat )
}
# Assign the agreement matrix to the object to plot
trust.agreement.dat <- dat.for.plot( trust.agreement )
# Plot the agreement scores/correlations
ggheatmap.T <- ggplot( data = trust.agreement.dat, aes( Var2, Var1, fill = value ) ) +
geom_tile( color = "white" ) +
scale_fill_gradient2( low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c( -1,1 ), space = "Lab",
name="Agreement" ) +
theme_minimal() +
coord_fixed() +
theme(
axis.title.x = element_blank(), axis.title.y = element_blank(),
axis.text.x=element_blank(), axis.text.y=element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
legend.justification = c(1, 0),
legend.position = c(0.6, 0.7),
legend.direction = "horizontal") +
guides(fill = guide_colorbar( barwidth = 7, barheight = 1,
title.position = "top", title.hjust = 0.5) ) +
ggtitle( "Plot of Agreement for Trust Items" )
print( ggheatmap.T )
# ================================================================== #
# CCA Models ----
# Create the model for the trust variables
trust.model <- ConsensusPipeline( SurveyResults = trust.dat, numQ = 2 )
library(AnthroTools)
# ================================================================== #
# CCA Models ----
# Create the model for the trust variables
trust.model <- ConsensusPipeline( SurveyResults = trust.dat, numQ = 2 )
trust.model
# Descriptive stats
summary( trust.model$origCompetence )
length( which( trust.model$origCompetence > 0.7 ) ) / 200
hist( trust.model$origCompetence )
# Histogram of the competencies
hist(
trust.model$origCompetence,
xlim=c( -1,1 ),
breaks = 15,
main = "Distribution of Cultural\n Embeddedness Scores",
xlab = "Cultural Embeddedness",
col= "lightblue"
)
abline( v = median( trust.model$origCompetence ), col = "blue", lwd = 3, lty = 2 )
abline( v = mean( trust.model$origCompetence ), col = "blue", lwd = 3, lty = 2 )
# ================================================================== #
# Clean up other variables for analysis ----
# Extract the relational health variables ----
# S3SS2_1-S3SS2_14
rh.vars <- as.matrix(
dat[,
which( colnames( dat ) == ( "S3SS2_1" ) ):
which( colnames( dat ) == ( "S3SS2_14" ) ) ] )
# replace 15 missing values: table( is.na( rh.vars ) )
rh.dat   <- NA.replace( rh.vars, round( runif( n = 1, min = 1, max = 5 ), 0 ) )
apply( rh.dat, 2, mean )
apply( rh.dat, 2, sd )
mean( apply( rh.dat, 2, mean ) )
sd( apply( rh.dat, 2, mean ) )
alpha( rh.dat )$total[1]
rh.dat
alpha( rh.dat )
alpha( rh.dat )$total[1]
# Extract the psychological safety variables ----
# S3SS3_1-S3SS3_7
ps.vars <- as.matrix(
dat[,
which( colnames( dat ) == ( "S3SS3_1" ) ):
which( colnames( dat ) == ( "S3SS3_7" ) ) ] )
# replace 15 missing values: table( is.na( ps.vars ) )
ps.dat   <- NA.replace( ps.vars, round( runif( n = 1, min = 1, max = 5 ), 0 ) )
apply( ps.dat, 2, mean )
apply( ps.dat, 2, sd )
mean( apply( ps.dat, 2, mean ) )
sd( apply( ps.dat, 2, mean ) )
alpha( ps.dat, check.keys = TRUE )$total[1]
# Create the objects to join ----
dat.for.models <- data.frame(
id = dat$id,
trust.competency = trust.model$origCompetence,
relhlth = apply( rh.dat, 1, mean ),
safety =  apply( ps.dat, 1, mean ),
interviewer = dat$InterviewerType,
randomized = dat$Randomize,
age = dat$S4Q1,
white = dat$white,
black = dat$black,
hispanic = dat$hispanic,
timein_yrs = dat$timein_yrs
)
